{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: when cleaning up, import data as modules "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data and Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../acled_data/acleddata_jan_july.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting fatalities to numeric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fatalities\"] = pd.to_numeric(df[\"fatalities\"], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"latitude\"] = pd.to_numeric(df[\"latitude\"], errors='coerce')\n",
    "df[\"longitude\"] = pd.to_numeric(df[\"longitude\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using spacy to extract data on wounded casualties from the 'notes' column and making a new feature - 'wounded' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the number of wounded from the notes column and adding a new column 'wounded'\n",
    "import spacy\n",
    "from word2number import w2n\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "def extract_wounded(text):\n",
    "    doc = nlp(text)\n",
    "    wounded_count = 0\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() == 'wounded':\n",
    "            for child in token.children:\n",
    "                if child.pos_ == 'NUM':\n",
    "                    try:\n",
    "                        wounded_count += int(child.text)\n",
    "                    except ValueError:\n",
    "                        wounded_count += w2n.word_to_num(child.text)\n",
    "            if wounded_count == 0:\n",
    "                for ancestor in token.ancestors:\n",
    "                    if ancestor.pos_ == 'NUM':\n",
    "                        try:\n",
    "                            wounded_count += int(ancestor.text)\n",
    "                        except ValueError:\n",
    "                            wounded_count += w2n.word_to_num(ancestor.text)\n",
    "                        break\n",
    "\n",
    "    return wounded_count\n",
    "\n",
    "df['wounded'] = df['notes'].apply(extract_wounded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1a: Parse event_date into a datetime format and extract month and year\n",
    "df[\"event_date\"] = pd.to_datetime(df[\"event_date\"])\n",
    "df[\"month\"] = df[\"event_date\"].dt.month\n",
    "df[\"year\"] = df[\"event_date\"].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining fatalities + wounded into casualties\n",
    "df['casualties'] = df['fatalities']+df['wounded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1c: Aggregate data by location and month\n",
    "# For example, we can sum fatalities and count the number of events for each location and month\n",
    "grouped_columns = [\"admin1\", \"admin2\", \"admin3\", \"location\", \"latitude\", \"longitude\", \"event_date\", \"year\", \"month\"]\n",
    "df_agg = df.groupby(grouped_columns).agg({\"casualties\": \"sum\", \"event_type\": \"count\"}).reset_index()\n",
    "df_agg = df_agg.rename(columns={\"event_type\": \"num_events\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought it would be a good idea to create a feature that will represent the proximity of locations to known hot zones where armed clashes occur, which in turn increases the chances of artillery shelling, drones, and missiles strikes affecting said areas. For this batch, I will limit the pool of hot zones to 4 locations: Bakhmut, Soledar, Avdiivka, Vuhledar. Those locations are known as battlefronts where frequent heavy armed clashes between opposing forces occur therefore anything close to them is very likely to be affected. \n",
    "\n",
    "In order to create this feature, I will compute the distance between locations in the data set to the hot zones by calculationg the distance between two sets of latitude and longitude points by using the Haversine formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Convert latitude and longitude to radians\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "    # Earth radius in kilometers\n",
    "    earth_radius = 6371\n",
    "\n",
    "    # Calculate the distance\n",
    "    distance = earth_radius * c\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define coordinates of the the battlefront towns\n",
    "bakhmut_coords = (48.5956, 37.9999)\n",
    "soledar_coords = (48.6833, 38.0667)\n",
    "avdiivka_coords = (48.1394, 37.7497)\n",
    "vuhledar_coords = (48.7798, 37.2490)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the haversine function to the dataset to calculate \n",
    "#the distances to each town and find the minimum distance:\n",
    "df_agg['distance_to_bakhmut'] = haversine(df_agg['latitude'], df_agg['longitude'], bakhmut_coords[0], bakhmut_coords[1])\n",
    "df_agg['distance_to_soledar'] = haversine(df_agg['latitude'], df_agg['longitude'], soledar_coords[0], soledar_coords[1])\n",
    "df_agg['distance_to_avdiivka'] = haversine(df_agg['latitude'], df_agg['longitude'], avdiivka_coords[0], avdiivka_coords[1])\n",
    "df_agg['distance_to_vuhledar'] = haversine(df_agg['latitude'], df_agg['longitude'], vuhledar_coords[0], vuhledar_coords[1])\n",
    "\n",
    "df_agg['min_distance_to_battlefront'] = df_agg[['distance_to_bakhmut', 'distance_to_soledar', 'distance_to_avdiivka', 'distance_to_vuhledar']].min(axis=1)\n",
    "\n",
    "# Drop the temporary distance columns\n",
    "df_agg = df_agg.drop(columns=['distance_to_bakhmut', 'distance_to_soledar', 'distance_to_avdiivka', 'distance_to_vuhledar'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the data with StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the variables\n",
    "scaler = StandardScaler()\n",
    "normalized_df_agg = scaler.fit_transform(df_agg[['casualties', 'num_events', 'min_distance_to_battlefront']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old hazard score feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hazard_score(row):\n",
    "    # Set the distance threshold (in km)\n",
    "    distance_threshold = 100\n",
    "    # Set the event count threshold\n",
    "    event_threshold = 10\n",
    "    \n",
    "    # Transform the distance variable using a nonlinear function\n",
    "    transformed_distance = np.exp(-row['min_distance_to_battlefront'] / 10)\n",
    "    \n",
    "    # Normalize fatalities, num_events, and transformed_distance\n",
    "    max_casualties = df_agg['casualties'].max()\n",
    "    max_num_events = df_agg['num_events'].max()\n",
    "    max_transformed_distance = df_agg['min_distance_to_battlefront'].apply(lambda x: np.exp(-x / 10)).max()\n",
    "    \n",
    "    normalized_casualties = row['casualties'] / max_casualties\n",
    "    normalized_num_events = row['num_events'] / max_num_events\n",
    "    normalized_transformed_distance = transformed_distance / max_transformed_distance\n",
    "    \n",
    "    # Define the weights for each variable\n",
    "    casualties_weight = 10\n",
    "    num_events_weight = 40\n",
    "    transformed_distance_weight = 50\n",
    "    \n",
    "    # Set the core hazard score based on the thresholds\n",
    "    core_hazard_score = 0\n",
    "    if row['min_distance_to_battlefront'] <= distance_threshold:\n",
    "        core_hazard_score += 60\n",
    "        if row['num_events'] >= event_threshold:\n",
    "            core_hazard_score += 20\n",
    "\n",
    "    elif row['num_events'] >= event_threshold:\n",
    "        core_hazard_score += 45\n",
    "    \n",
    "    elif row['num_events'] >= 5:\n",
    "        core_hazard_score += 35\n",
    "    \n",
    "    elif row['num_events'] >= 1 and row['num_events']<5:\n",
    "        core_hazard_score += 20\n",
    "\n",
    "    if row['min_distance_to_battlefront'] <= 200 and row['min_distance_to_battlefront']>distance_threshold:\n",
    "        core_hazard_score += 30\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Calculate the hazard score based on the core score and the weighted normalized values\n",
    "    hazard_score = (\n",
    "        core_hazard_score +\n",
    "        normalized_casualties * casualties_weight +\n",
    "        normalized_num_events * num_events_weight +\n",
    "        normalized_transformed_distance * transformed_distance_weight\n",
    "    )\n",
    "    \n",
    "    # Ensure the hazard score does not exceed 100\n",
    "    hazard_score = min(hazard_score, 100)\n",
    "    \n",
    "    return hazard_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_scores = {}\n",
    "for index, row in df_agg.iterrows():\n",
    "    location = row['location']\n",
    "    hazard_score = calculate_hazard_score(row)\n",
    "    hazard_scores[location] = calculate_hazard_score(row)\n",
    "    df_agg.loc[index, 'hazard_score']=hazard_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hazard score feature PCA variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(normalized_df_agg)\n",
    "\n",
    "# Calculate the hazard scores using the first principal component\n",
    "hazard_scores = pca.transform(normalized_df_agg)\n",
    "\n",
    "# Scale the hazard scores to a range of 0 to 100\n",
    "scaled_hazard_scores = (hazard_scores - hazard_scores.min()) / (hazard_scores.max() - hazard_scores.min()) * 100\n",
    "\n",
    "# Add the scaled hazard scores to the original dataframe\n",
    "df_agg['hazard_score_pca'] = scaled_hazard_scores\n",
    "\n",
    "# Create a dictionary of hazard scores for each location\n",
    "hazard_scores_pca = dict(zip(df_agg['location'], df_agg['hazard_score_pca']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out multiple PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2  # or any other number you want to use\n",
    "\n",
    "# Normalize the variables\n",
    "scaler = StandardScaler()\n",
    "normalized_df_agg = scaler.fit_transform(df_agg[['casualties', 'num_events', 'min_distance_to_battlefront']])\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(normalized_df_agg)\n",
    "\n",
    "# Calculate the hazard scores using the first principal component\n",
    "hazard_scores = pca.transform(normalized_df_agg)\n",
    "\n",
    "#Compute weighted hazard scores\n",
    "weighted_hazard_scores = np.dot(hazard_scores, pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "# Scale the hazard scores to a range of 0 to 100\n",
    "scaled_weighted_hazard_scores = (weighted_hazard_scores - weighted_hazard_scores.min()) / (weighted_hazard_scores.max() - weighted_hazard_scores.min()) * 100\n",
    "\n",
    "# Add the scaled hazard scores to the original dataframe\n",
    "df_agg['hazard_score_pca'] = scaled_weighted_hazard_scores\n",
    "\n",
    "# Create a dictionary of hazard scores for each location\n",
    "hazard_scores_pca = dict(zip(df_agg['location'], df_agg['hazard_score_pca']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_hazard_score(row):\n",
    "    distance_threshold = 100\n",
    "    event_threshold = 10\n",
    "\n",
    "    core_hazard_score = 0\n",
    "    \n",
    "    # Apply adjustments based on distance to the battlefront\n",
    "    if row['min_distance_to_battlefront'] <= distance_threshold:\n",
    "        core_hazard_score += 60\n",
    "        if row['num_events'] >= event_threshold:\n",
    "            core_hazard_score += 20\n",
    "    elif row['min_distance_to_battlefront'] <= 200:\n",
    "        core_hazard_score += 30\n",
    "    \n",
    "    # Apply adjustments based on the number of events\n",
    "    if row['num_events'] >= event_threshold:\n",
    "        core_hazard_score += 45\n",
    "    elif row['num_events'] >= 5:\n",
    "        core_hazard_score += 35\n",
    "    elif row['num_events'] >= 1:\n",
    "        core_hazard_score += 20\n",
    "\n",
    "    hazard_score = row['hazard_score_pca'] + core_hazard_score\n",
    "    hazard_score = min(hazard_score, 100)\n",
    "    \n",
    "    return hazard_score\n",
    "\n",
    "# Apply the adjustments to the PCA-based hazard scores\n",
    "df_agg['pca_hazard_score'] = df_agg.apply(pca_hazard_score, axis=1)\n",
    "\n",
    "df_agg = df_agg.drop('hazard_score_pca', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the explained variance ratio for the first principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Explained variance ratio:\", explained_variance_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hazard score TSNE variant - currently selected model for hazard score modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform t-SNE\n",
    "tsne = TSNE(n_components=1, random_state=42)\n",
    "embedded_data = tsne.fit_transform(normalized_df_agg)\n",
    "\n",
    "# Scale the t-SNE embeddings to a range of 0 to 100\n",
    "scaled_embedded_data = (embedded_data - embedded_data.min()) / (embedded_data.max() - embedded_data.min()) * 100\n",
    "\n",
    "df_agg['hazard_score_tsne'] = scaled_embedded_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between PCA and weighted sum hazard scores: 0.9338408991790772\n"
     ]
    }
   ],
   "source": [
    "# Calculate the correlation between the adjusted PCA hazard scores and the weighted sum hazard scores\n",
    "correlation = df_agg['pca_hazard_score'].corr(df_agg['hazard_score'])\n",
    "print(\"Correlation between PCA and weighted sum hazard scores:\", correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between PCA and tsne hazard scores: 0.8287615694458589\n"
     ]
    }
   ],
   "source": [
    "# Calculate the correlation between the adjusted PCA hazard scores and the tsne hazard scores\n",
    "correlation = df_agg['pca_hazard_score'].corr(df_agg['hazard_score_tsne'])\n",
    "print(\"Correlation between PCA and tsne hazard scores:\", correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between tsne and weighted sum hazard scores: 0.886348335529343\n"
     ]
    }
   ],
   "source": [
    "# Calculate the correlation between the adjusted tsne hazard scores and the weighted sum hazard scores\n",
    "correlation = df_agg['hazard_score_tsne'].corr(df_agg['hazard_score'])\n",
    "print(\"Correlation between tsne and weighted sum hazard scores:\", correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting data to handle inconsistencies with naming and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.loc[df_agg['admin2']=='Kyiv', 'admin3'] = 'Kyiv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.loc[df_agg['location']=='Kherson', 'admin2'] = 'Khersonskyi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df_agg[~df_agg['admin3'].str.strip().eq('')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 23281 entries, 0 to 23287\n",
      "Data columns (total 13 columns):\n",
      " #   Column                       Non-Null Count  Dtype         \n",
      "---  ------                       --------------  -----         \n",
      " 0   admin1                       23281 non-null  object        \n",
      " 1   admin2                       23281 non-null  object        \n",
      " 2   admin3                       23281 non-null  object        \n",
      " 3   location                     23281 non-null  object        \n",
      " 4   latitude                     23281 non-null  float64       \n",
      " 5   longitude                    23281 non-null  float64       \n",
      " 6   event_date                   23281 non-null  datetime64[ns]\n",
      " 7   year                         23281 non-null  int32         \n",
      " 8   month                        23281 non-null  int32         \n",
      " 9   casualties                   23281 non-null  int64         \n",
      " 10  num_events                   23281 non-null  int64         \n",
      " 11  min_distance_to_battlefront  23281 non-null  float64       \n",
      " 12  hazard_score_tsne            23281 non-null  float32       \n",
      "dtypes: datetime64[ns](1), float32(1), float64(3), int32(2), int64(2), object(4)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_agg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.to_csv('Hazards_latest.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
